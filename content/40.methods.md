## Methods

BioChatter is a Python library, currently supporting Python 3.10-3.12, and generally the three most recent releases, which we ensure with a continuous integration pipeline on GitHub.
ChatGSE is a web app based on the Streamlit framework (version 1.21.0, https://streamlit.io), which is written in Python and can be deployed locally or on a server (https://github.com/biocypher/ChatGSE).
It is mainly used for demonstrating the various applications of the BioChatter framework and API.
<!-- TODO update web app, preview -->
For an up-to-date overview and preview of current functionality of the platform, please visit the [online preview](https://chat.biocypher.org).
ChatGSE Next (https://github.com/biocypher/chatgse-next) is a modern web app with server-client architecture, based on the open-source template of ChatGPT-Next-Web (https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web).
It is written in TypeScript using Flask and Node.js, and demonstrates the use of BioChatter in a modern web app.
All packages are developed openly and according to modern standards of software development [@doi:10.1038/s41597-020-0486-7]; we use the permissive MIT licence to encourage downstream use and development.
We include a code of conduct and contributor guidelines to offer accessibility and inclusivity to all that are interested in contributing to the framework.

### Benchmarking

The benchmarking framework implements a matrix of component combinations using the parameterisation feature of Pytest [@pytest].
This allows the automated evaluation of all possible combinations of components, such as LLMs, prompts, and datasets.
The results are stored in a database and displayed on the website for easy comparison.
The benchmark is updated upon the release of new models and extensions to the datasets.
The individual dimensions of the matrix are:

- **LLMs**: proprietary (OpenAI) and open-source models (commonly using the Xorbits Inference API and HuggingFace models)

- **prompts**: a set of prompts for each task with varying degrees of specificity and fixed as well as variable components

- **datasets**: a set of datasets for each task, including the BioChatter benchmark datasets and some general purpose LLM benchmark datasets

- **model quantisations**: a set of quantisations for each model (where available), to account for the trade-off between model size and performance

- **model parameters**: a set of parameters for each model, such as temperature

- **integrations**: specific tasks that require integrations, for instance with knowledge graphs or vector databases, are tested in a dedicated fashion

The Pytest framework is implemented at https://github.com/biocypher/biochatter/blob/main/benchmark/, and more information and results are available at https://biocypher.github.io/biochatter/benchmark/.

To prevent leakage of benchmarking data (and subsequent contamination of future LLMs), we implement an encryption routine on the benchmark datasets.
The encryption is performed using a hybrid encryption scheme, where the data is encrypted with a symmetric key, which is in turn encrypted with an asymmetric key.
The datasets are stored in a dedicated encrypted pipeline that is only accessible to the workflow that executes the benchmark.
These processes are implemented at https://github.com/biocypher/llm-test-dataset/ and accessed from the benchmark procedure in BioChatter.

### Knowledge Graphs

### Retrieval Augmented Generation

### Model Chaining

### Remnants

These interactions are handled by a pre-programmed conversational “Assistant,” which dynamically orchestrates LLM agents with distinct tasks using a Python model chaining framework [@{https://python.langchain.com}].
Using vector database approaches, the user’s prompts can be further supplemented with information extracted from pertinent, user-provided literature (Supplementary Note [In-context Learning]).

For example, connecting to a knowledge graph of cell markers based on Cell Ontology [doi:10.1186/s13326-016-0088-7], the task of annotating single cell data sets can be automated and made more reproducible (Supplementary Note [Cell Type Annotation]) by abstracting the pioneering efforts of manually executed studies [@doi:10.1101/2023.04.16.537094].

Using the emergent strategies of in-context learning, instruction learning, and chain-of-thought prompting [@doi:10.48550/arxiv.2303.17580], this can enable causal inference on relationships between biological entities, for instance via protein-protein interactions (Supplementary Note [Causal Inference]), and automated validation of literature references provided by the LLM (Supplementary Note [Literature Reference Database]).
The ability to chain arbitrary types of models enables advanced applications, for instance connecting to visual modalities such as spatial omics.
