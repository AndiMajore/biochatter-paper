## Methods

On the model side, we implement several measures in addition to the prompt engineering around the user's queries.
For instance, we deploy a second model to safeguard the factual correctness of the primary LLM's responses (Supplementary Note [Correcting Agent]).
These interactions are handled by a pre-programmed conversational “Assistant,” which dynamically orchestrates LLM agents with distinct tasks using a Python model chaining framework [@{https://python.langchain.com}].
Using vector database approaches, the user’s prompts can be further supplemented with information extracted from pertinent, user-provided literature (Supplementary Note [In-context Learning]).

To increase data-awareness of the AI agents, we introduce connectivity to databases, which can extend the long-term memory of the models, semantically ground the biological entities with respect to suitable ontologies, and compare the model's responses to prior knowledge ground truth.
By integrating a flexible knowledge graph creation framework [@doi:10.1038/s41587-023-01848-y], we allow versatile use cases across the entire research spectrum.
For example, connecting to a knowledge graph of cell markers based on Cell Ontology [doi:10.1186/s13326-016-0088-7], the task of annotating single cell data sets can be automated and made more reproducible (Supplementary Note [Cell Type Annotation]) by abstracting the pioneering efforts of manually executed studies [@doi:10.1101/2023.04.16.537094].
We engineer the prompts around knowledge graph queries around the semantic configuration of BioCypher knowledge graphs, which allows efficient communication of the LLM with the knowledge graph and avoids hallucination-based issues.
This allows querying any knowledge graph using natural language as well as the development of advanced multi-stage reasoning strategies (Supplementary Note [Knowledge Graphs]).

In the future, we aim to integrate biological prior knowledge representation with LLM reasoning.
Using the emergent strategies of in-context learning, instruction learning, and chain-of-thought prompting [@doi:10.48550/arxiv.2303.17580], this can enable causal inference on relationships between biological entities, for instance via protein-protein interactions (Supplementary Note [Causal Inference]), and automated validation of literature references provided by the LLM (Supplementary Note [Literature Reference Database]).
While the current models do not yet appear suited for unsupervised reasoning in the biomedical space, they can already save much time otherwise spent on web and literature searches.
Additionally, the ChatGSE platform provides a reproducible environment for benchmarking of models and engineered prompts to gauge their biomedical reliability.
The ability to chain arbitrary types of models enables advanced applications, for instance connecting to visual modalities such as spatial omics.
We provide further details and application scenarios in our Supplementary Notes.

While we focus on the biomedical field, the concept of the tool can easily be extended to other scientific domains by adjusting domain-specific prompts and data inputs, which in our framework are accessible in a composable and user-friendly manner.
The Python library to interact with LLMs, vector databases, and all other features is developed openly on GitHub (https://github.com/biocypher/biochatter), and can be integrated into any number of user interface solutions apart from our own, for instance, INDRA [@doi:10.15252/msb.20177651] and drugst.one [@doi:10.48550/arxiv.2305.15453].
We develop under the permissive MIT licence and encourage contributions and suggestions from the community with regard to the addition of bioinformatics tool inputs, prompt engineering, safeguarding mechanisms, and any other feature.