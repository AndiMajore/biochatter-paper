## Methods

BioChatter is a Python library, currently supporting Python 3.10-3.12, and generally the three most recent releases, which we ensure with a continuous integration pipeline on GitHub.
ChatGSE is a web app based on the Streamlit framework (version 1.21.0, https://streamlit.io), which is written in Python and can be deployed locally or on a server (https://github.com/biocypher/ChatGSE).
It is mainly used for demonstrating the various applications of the BioChatter framework and API.
<!-- TODO update web app, preview -->
For an up-to-date overview and preview of current functionality of the platform, please visit the [online preview](https://chat.biocypher.org).
ChatGSE Next (https://github.com/biocypher/chatgse-next) is a modern web app with server-client architecture, based on the open-source template of ChatGPT-Next-Web (https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web).
It is written in TypeScript using Flask and Node.js, and demonstrates the use of BioChatter in a modern web app.
All packages are developed openly and according to modern standards of software development [@doi:10.1038/s41597-020-0486-7]; we use the permissive MIT licence to encourage downstream use and development.
We include a code of conduct and contributor guidelines to offer accessibility and inclusivity to all that are interested in contributing to the framework.

### Benchmarking

The benchmarking framework implements a matrix of component combinations using the parameterisation feature of Pytest [@pytest].
This allows the automated evaluation of all possible combinations of components, such as LLMs, prompts, and datasets.
The results are stored in a database and displayed on the website for easy comparison.
The benchmark is updated upon the release of new models and extensions to the datasets.
The individual dimensions of the matrix are:

- **LLMs**: proprietary (OpenAI) and open-source models (commonly using the Xorbits Inference API and HuggingFace models)

- **prompts**: a set of prompts for each task with varying degrees of specificity and fixed as well as variable components

- **datasets**: a set of datasets for each task, including the BioChatter benchmark datasets and some general purpose LLM benchmark datasets

- **data processing**: non-AI data processing steps, such as the conversion of numbers (which LLMs are notoriously bad at handling) to categorical text (e.g., low, medium, high)

- **model quantisations**: a set of quantisations for each model (where available), to account for the trade-off between model size and performance

- **model parameters**: a set of parameters for each model, such as temperature

- **integrations**: specific tasks that require integrations, for instance with knowledge graphs or vector databases, are tested in a dedicated fashion

The Pytest framework is implemented at https://github.com/biocypher/biochatter/blob/main/benchmark/, and more information and results are available at https://biocypher.github.io/biochatter/benchmark/.

To prevent leakage of benchmarking data (and subsequent contamination of future LLMs), we implement an encryption routine on the benchmark datasets.
The encryption is performed using a hybrid encryption scheme, where the data is encrypted with a symmetric key, which is in turn encrypted with an asymmetric key.
The datasets are stored in a dedicated encrypted pipeline that is only accessible to the workflow that executes the benchmark.
These processes are implemented at https://github.com/biocypher/llm-test-dataset/ and accessed from the benchmark procedure in BioChatter.

### Knowledge Graphs

### Retrieval Augmented Generation

While the general knowledge of current LLMs is extensive, they may not know how to prioritise very specific scientific results, or they may not have had access to some research articles in their training data (e.g., due to their recency or licensing issues).
To bridge this gap, we can provide additional information from relevant publications to the model via the prompt.
However, we cannot add entire publications to the prompt, since the input length of current models still is restricted; we need to isolate the information that is specifically relevant to the question given by the user.
To find this information, we perform a semantic similarity search between the user’s question and the contents of user-provided scientific articles (or other texts).
The most efficient way to do this mapping is by using a vector database.

The contextual background information provided by the user (e.g., by uploading a scientific article of prior work related to the experiment to be interpreted) is split into pieces suitable to be digested by the LLM, which are individually embedded by the model.
These embeddings (represented by vectors) are used to store the text fragments in a vector database; the storage as vectors allows fast and efficient retrieval of similar entities via the comparison of individual vectors.
For example, the two sentences “Amyloid beta levels are associated with Alzheimer’s Disease stage.” and “One of the most important clinical markers of AD progression is the amount of deposited A-beta 42.” would be closely associated in a vector database (given the embedding model is of sufficient quality, i.e., similar to GPT-3 or better), while traditional text-based similarity metrics probably would not identify them as highly similar.

By comparing the user’s question to prior knowledge in the vector database, we can extract the relevant pieces of information from the entire background.
Even better, we can first use an LLM to generate an answer to the user's question and then use this answer to query the vector database for relevant information.
Regardless of whether the initial answer is correct, it is likely that the "fake answer" is more semantically similar to the relevant pieces of information than the user's question [@doi:10.48550/arXiv.2308.07107].
These pieces (for instance, single sentences directly related to the topic of the question) are then sufficiently small to be added to the prompt.
In this way, the model can learn from additional context without the need for retraining or fine-tuning.
This method is sometimes described as in-context learning [@doi:10.48550/arxiv.2303.17580] or retrieval-augmented generation [@rag].

To provide access to this functionality in BioChatter, we implement classes for the connection to and management of vector database systems (in the vectorstore_host.py module) and for performing semantic search on the vector database and injecting the results into the prompt (in the vectorstore.py module).
To demonstrate the use of the API, we add a “Retrieval Augmented Generation” tab to the ChatGSE preview app that allows the upload of text documents to be added to a vector database, which then can be queried to add contextual information to the prompt sent to the primary model.
This contextual information is transparently displayed in the main chat tab.
Since this functionality requires a connection to a vector database system, we provide connectivity to a Milvus server, including a way to start the server in conjunction with a BioCypher knowledge graph and the ChatGSE app in one Docker Compose workflow.
We plan to extend vector database support to several other standard vector database providers, such as Pinecone and Weaviate.

### Deployment of Open-Source Models

### Model Chaining

### Remnants

These interactions are handled by a pre-programmed conversational “Assistant,” which dynamically orchestrates LLM agents with distinct tasks using a Python model chaining framework [@{https://python.langchain.com}].
Using vector database approaches, the user’s prompts can be further supplemented with information extracted from pertinent, user-provided literature (Supplementary Note [In-context Learning]).

For example, connecting to a knowledge graph of cell markers based on Cell Ontology [doi:10.1186/s13326-016-0088-7], the task of annotating single cell data sets can be automated and made more reproducible (Supplementary Note [Cell Type Annotation]) by abstracting the pioneering efforts of manually executed studies [@doi:10.1101/2023.04.16.537094].

Using the emergent strategies of in-context learning, instruction learning, and chain-of-thought prompting [@doi:10.48550/arxiv.2303.17580], this can enable causal inference on relationships between biological entities, for instance via protein-protein interactions (Supplementary Note [Causal Inference]), and automated validation of literature references provided by the LLM (Supplementary Note [Literature Reference Database]).
The ability to chain arbitrary types of models enables advanced applications, for instance connecting to visual modalities such as spatial omics.
